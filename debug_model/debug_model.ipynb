{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pyuvdata\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fixing auto-correlations to be be real-only, after some imaginary values were detected in data_array. Largest imaginary component was 1.5049646242621317e-10, largest imaginary/real ratio was 1.1618951822369998e-15.\n",
      "Fixing auto-correlations to be be real-only, after some imaginary values were detected in data_array. Largest imaginary component was 1.292133371769155e-10, largest imaginary/real ratio was 1.3277846097372624e-15.\n",
      "Fixing auto-correlations to be be real-only, after some imaginary values were detected in data_array. Largest imaginary component was 1.4157853988361357e-10, largest imaginary/real ratio was 1.4752735754290612e-15.\n",
      "Fixing auto-correlations to be be real-only, after some imaginary values were detected in data_array. Largest imaginary component was 1.4640716982809504e-10, largest imaginary/real ratio was 1.4994253786609909e-15.\n",
      "Fixing auto-correlations to be be real-only, after some imaginary values were detected in data_array. Largest imaginary component was 1.2700829244653017e-10, largest imaginary/real ratio was 1.3585179771712085e-15.\n",
      "Fixing auto-correlations to be be real-only, after some imaginary values were detected in data_array. Largest imaginary component was 1.1007317421161456e-10, largest imaginary/real ratio was 1.6750151058574563e-15.\n",
      "Fixing auto-correlations to be be real-only, after some imaginary values were detected in data_array. Largest imaginary component was 1.4705459461968854e-10, largest imaginary/real ratio was 1.2206582843006333e-15.\n",
      "Fixing auto-correlations to be be real-only, after some imaginary values were detected in data_array. Largest imaginary component was 1.4504088420003018e-10, largest imaginary/real ratio was 1.2094540851038082e-15.\n",
      "Fixing auto-correlations to be be real-only, after some imaginary values were detected in data_array. Largest imaginary component was 1.2924019599027954e-10, largest imaginary/real ratio was 1.4193844186880168e-15.\n",
      "Fixing auto-correlations to be be real-only, after some imaginary values were detected in data_array. Largest imaginary component was 1.4897836895379728e-10, largest imaginary/real ratio was 1.1542819552607338e-15.\n",
      "Fixing auto-correlations to be be real-only, after some imaginary values were detected in data_array. Largest imaginary component was 1.227998581388389e-10, largest imaginary/real ratio was 1.5445615900413131e-15.\n",
      "Fixing auto-correlations to be be real-only, after some imaginary values were detected in data_array. Largest imaginary component was 1.5228813360774725e-10, largest imaginary/real ratio was 1.3585761536503578e-15.\n",
      "Fixing auto-correlations to be be real-only, after some imaginary values were detected in data_array. Largest imaginary component was 1.1307341928691638e-10, largest imaginary/real ratio was 1.5560674836785315e-15.\n",
      "Fixing auto-correlations to be be real-only, after some imaginary values were detected in data_array. Largest imaginary component was 1.1158199844578756e-10, largest imaginary/real ratio was 1.5935684899343434e-15.\n",
      "Fixing auto-correlations to be be real-only, after some imaginary values were detected in data_array. Largest imaginary component was 1.293013250451818e-10, largest imaginary/real ratio was 1.3922778687809833e-15.\n",
      "Fixing auto-correlations to be be real-only, after some imaginary values were detected in data_array. Largest imaginary component was 1.1238731556249067e-10, largest imaginary/real ratio was 1.6414124798190796e-15.\n",
      "Fixing auto-correlations to be be real-only, after some imaginary values were detected in data_array. Largest imaginary component was 1.2716076798408363e-10, largest imaginary/real ratio was 1.7360725402864312e-15.\n",
      "Fixing auto-correlations to be be real-only, after some imaginary values were detected in data_array. Largest imaginary component was 1.1812571293980155e-10, largest imaginary/real ratio was 1.6403481644620823e-15.\n",
      "Fixing auto-correlations to be be real-only, after some imaginary values were detected in data_array. Largest imaginary component was 1.4797776598214299e-10, largest imaginary/real ratio was 1.3116356827229094e-15.\n",
      "Fixing auto-correlations to be be real-only, after some imaginary values were detected in data_array. Largest imaginary component was 1.529000470647994e-10, largest imaginary/real ratio was 1.3306492654784096e-15.\n",
      "Fixing auto-correlations to be be real-only, after some imaginary values were detected in data_array. Largest imaginary component was 1.4511592626067615e-10, largest imaginary/real ratio was 1.2320411778407857e-15.\n",
      "Fixing auto-correlations to be be real-only, after some imaginary values were detected in data_array. Largest imaginary component was 1.2817186346795182e-10, largest imaginary/real ratio was 1.3877861625565975e-15.\n",
      "Fixing auto-correlations to be be real-only, after some imaginary values were detected in data_array. Largest imaginary component was 1.4628427386775285e-10, largest imaginary/real ratio was 1.1279498555395248e-15.\n",
      "Fixing auto-correlations to be be real-only, after some imaginary values were detected in data_array. Largest imaginary component was 1.17624081786736e-10, largest imaginary/real ratio was 1.574848863686158e-15.\n",
      "Fixing auto-correlations to be be real-only, after some imaginary values were detected in data_array. Largest imaginary component was 1.0778114086789519e-10, largest imaginary/real ratio was 1.6745317733737442e-15.\n",
      "Fixing auto-correlations to be be real-only, after some imaginary values were detected in data_array. Largest imaginary component was 1.0805875911136607e-10, largest imaginary/real ratio was 1.7010191238226891e-15.\n",
      "Fixing auto-correlations to be be real-only, after some imaginary values were detected in data_array. Largest imaginary component was 1.0995032646523477e-10, largest imaginary/real ratio was 1.6341394061122588e-15.\n",
      "Fixing auto-correlations to be be real-only, after some imaginary values were detected in data_array. Largest imaginary component was 1.0652727274108225e-10, largest imaginary/real ratio was 1.61393045257567e-15.\n",
      "Fixing auto-correlations to be be real-only, after some imaginary values were detected in data_array. Largest imaginary component was 1.4037471430747035e-10, largest imaginary/real ratio was 1.4579794579030939e-15.\n",
      "Fixing auto-correlations to be be real-only, after some imaginary values were detected in data_array. Largest imaginary component was 1.4186852953117007e-10, largest imaginary/real ratio was 1.5548657360052724e-15.\n",
      "Fixing auto-correlations to be be real-only, after some imaginary values were detected in data_array. Largest imaginary component was 1.3867901023260703e-10, largest imaginary/real ratio was 1.4472091990678364e-15.\n",
      "Fixing auto-correlations to be be real-only, after some imaginary values were detected in data_array. Largest imaginary component was 1.2455563286366744e-10, largest imaginary/real ratio was 1.4408394512944737e-15.\n",
      "Fixing auto-correlations to be be real-only, after some imaginary values were detected in data_array. Largest imaginary component was 1.277003564716022e-10, largest imaginary/real ratio was 1.3065636191603083e-15.\n",
      "Fixing auto-correlations to be be real-only, after some imaginary values were detected in data_array. Largest imaginary component was 1.3674639344966432e-10, largest imaginary/real ratio was 1.4621043448149339e-15.\n",
      "Fixing auto-correlations to be be real-only, after some imaginary values were detected in data_array. Largest imaginary component was 1.1383488362413886e-10, largest imaginary/real ratio was 1.5703423871874963e-15.\n"
     ]
    }
   ],
   "source": [
    "# Get model LSTS\n",
    "model_filepath = \"/safepool/rbyrne/hera_data/H4C-Abscal-Model\"\n",
    "model_filenames = os.listdir(model_filepath)\n",
    "model_lsts = []\n",
    "model_lst_filenames = []\n",
    "for model_file in model_filenames:\n",
    "    model_uv = pyuvdata.UVData()\n",
    "    model_uv.read(f\"{model_filepath}/{model_file}\")\n",
    "    lsts_new = list(set(model_uv.lst_array))\n",
    "    model_lsts = np.concatenate((model_lsts, lsts_new))\n",
    "    filenames_new = np.repeat([model_file], len(lsts_new))\n",
    "    model_lst_filenames = np.concatenate((model_lst_filenames, filenames_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_uv = pyuvdata.UVData()\n",
    "data_uv.read(\n",
    "    \"/safepool/rbyrne/hera_data/H6C-data/2459861/zen.2459861.45004.sum.abs_calibrated.red_avg.uvh5\"\n",
    ")\n",
    "\n",
    "# Confirm that model files exist for the given data\n",
    "use_file = True\n",
    "for use_lst in list(set(data_uv.lst_array)):\n",
    "    if use_lst <= np.min(model_lsts):\n",
    "        use_lst += 2 * np.pi\n",
    "    elif use_lst >= np.max(model_lsts):\n",
    "        use_lst -= 2 * np.pi\n",
    "    lst_distance = np.abs(model_lsts - use_lst)\n",
    "    if np.sort(lst_distance)[1] > 0.0015 / 2:  # Model for the file LST does not exist\n",
    "        use_file = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Missing some redundant groups. Filling in available data.\n",
      "The use_grid_alg parameter is not set. Defaulting to True to use the new gridding based algorithm (developed by the HERA team) rather than the older clustering based algorithm. This is change to the default, to use the clustering algorithm set use_grid_alg=False.\n"
     ]
    }
   ],
   "source": [
    "data = data_uv.select(lsts=use_lst, inplace=False)\n",
    "data.inflate_by_redundancy(use_grid_alg=True)\n",
    "red_baselines, _, _, conj_red_baselines = data.get_redundancies(include_conjugates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_antenna_pair_from_baseline_nums(baseline_nums):\n",
    "    ant2 = (baseline_nums - 2**16)%2048\n",
    "    ant1 = [int(value) for value in (baseline_nums - 2**16 - ant2)/2048]\n",
    "    return list(zip(ant1, ant2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zen.2458894.09630.uvh5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Missing some redundant groups. Filling in available data.\n",
      "The use_grid_alg parameter is not set. Defaulting to True to use the new gridding based algorithm (developed by the HERA team) rather than the older clustering based algorithm. This is change to the default, to use the clustering algorithm set use_grid_alg=False.\n",
      "invalid value encountered in cast\n",
      "Fixing auto-correlations to be be real-only, after some imaginary values were detected in data_array. Largest imaginary component was 1.5228813360774725e-10, largest imaginary/real ratio was 1.3585761536503578e-15.\n",
      "invalid value encountered in cast\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 47\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m bl_ind, baseline \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(model\u001b[38;5;241m.\u001b[39mant_1_array, model\u001b[38;5;241m.\u001b[39mant_2_array))\n\u001b[1;32m     45\u001b[0m ):\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m red_ind, red_group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(red_baselines):\n\u001b[0;32m---> 47\u001b[0m         red_group_ants \u001b[38;5;241m=\u001b[39m \u001b[43mget_antenna_pair_from_baseline_nums\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mred_group\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m baseline \u001b[38;5;129;01min\u001b[39;00m red_group_ants:\n\u001b[1;32m     51\u001b[0m             red_groups_model[bl_ind] \u001b[38;5;241m=\u001b[39m red_ind\n",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m, in \u001b[0;36mget_antenna_pair_from_baseline_nums\u001b[0;34m(baseline_nums)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_antenna_pair_from_baseline_nums\u001b[39m(baseline_nums):\n\u001b[1;32m      2\u001b[0m     ant2 \u001b[38;5;241m=\u001b[39m (baseline_nums \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m16\u001b[39m)\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m2048\u001b[39m\n\u001b[0;32m----> 3\u001b[0m     ant1 \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mint\u001b[39m(value) \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m (baseline_nums \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m16\u001b[39m \u001b[38;5;241m-\u001b[39m ant2)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2048\u001b[39m]\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(ant1, ant2))\n",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_antenna_pair_from_baseline_nums\u001b[39m(baseline_nums):\n\u001b[1;32m      2\u001b[0m     ant2 \u001b[38;5;241m=\u001b[39m (baseline_nums \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m16\u001b[39m)\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m2048\u001b[39m\n\u001b[0;32m----> 3\u001b[0m     ant1 \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mint\u001b[39m(value) \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m (baseline_nums \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m16\u001b[39m \u001b[38;5;241m-\u001b[39m ant2)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2048\u001b[39m]\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(ant1, ant2))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if use_file:  # Keep only files where all LSTs are covered by the model\n",
    "    model_uv_list = []\n",
    "    for time_ind, use_lst in enumerate(list(set(data_uv.lst_array))):  # Iterate over LSTs\n",
    "        lst_distance = np.abs(model_lsts - use_lst)\n",
    "        ind1 = np.where(lst_distance == np.min(lst_distance))[0]  # Use nearest neighbor model LST\n",
    "        # ind2 = np.where(lst_distance == np.sort(lst_distance)[1])[0]\n",
    "        lst1 = model_lsts[ind1]\n",
    "        model_filename1 = model_lst_filenames[ind1][0]\n",
    "        print(model_filename1)\n",
    "\n",
    "        data = data_uv.select(lsts=use_lst, polarizations=[-5], inplace=False)\n",
    "        data.conjugate_bls()\n",
    "        data.inflate_by_redundancy(use_grid_alg=True)\n",
    "        red_baselines, _, _, conj_red_baselines = data.get_redundancies(include_conjugates=True)\n",
    "        data.compress_by_redundancy()\n",
    "        data.phase_to_time(np.mean(data.time_array))\n",
    "\n",
    "        red_groups_data = np.full((data.Nbls), np.nan, int)\n",
    "        for bl_ind, baseline in enumerate(list(zip(data.ant_1_array, data.ant_2_array))):\n",
    "            for red_ind, red_group in enumerate(red_baselines):\n",
    "                red_group_ants = get_antenna_pair_from_baseline_nums(np.array(red_group))\n",
    "                if baseline in red_group_ants:\n",
    "                    red_groups_data[bl_ind] = red_ind\n",
    "                elif baseline[::-1] in red_group_ants:\n",
    "                    ant1 = data.ant_2_array[bl_ind]\n",
    "                    ant2 = data.ant_1_array[bl_ind]\n",
    "                    data.baseline_array[bl_ind] = 2048*ant1 + ant2 + 2**16\n",
    "                    data.data_array[bl_ind, :, :] = np.conj(\n",
    "                        data.data_array[bl_ind, :, :]\n",
    "                    )\n",
    "                    data.ant_1_array[bl_ind] = ant2\n",
    "                    data.ant_2_array[bl_ind] = ant1\n",
    "                    red_groups_data[bl_ind] = red_ind\n",
    "\n",
    "        model = pyuvdata.UVData()\n",
    "        model.read(f\"{model_filepath}/{model_filename1}\")\n",
    "        model.conjugate_bls()\n",
    "        model.compress_by_redundancy()\n",
    "        model.select(lsts=model_lsts[np.where(lst_distance == np.min(lst_distance))[0]], polarizations=[-5], inplace=True)\n",
    "        model.phase_to_time(np.mean(data.time_array))\n",
    "\n",
    "        red_groups_model = np.full((model.Nbls), np.nan, int)\n",
    "        for bl_ind, baseline in enumerate(\n",
    "            list(zip(model.ant_1_array, model.ant_2_array))\n",
    "        ):\n",
    "            for red_ind, red_group in enumerate(red_baselines):\n",
    "                red_group_ants = get_antenna_pair_from_baseline_nums(\n",
    "                    np.array(red_group)\n",
    "                )\n",
    "                if baseline in red_group_ants:\n",
    "                    red_groups_model[bl_ind] = red_ind\n",
    "                    continue\n",
    "                elif baseline[::-1] in red_group_ants:\n",
    "                    ant1 = model.ant_2_array[bl_ind]\n",
    "                    ant2 = model.ant_1_array[bl_ind]\n",
    "                    model.baseline_array[bl_ind] = 2048 * ant1 + ant2 + 2**16\n",
    "                    model.data_array[bl_ind, :, :] = np.conj(\n",
    "                        model.data_array[bl_ind, :, :]\n",
    "                    )\n",
    "                    model.ant_1_array[bl_ind] = ant2\n",
    "                    model.ant_2_array[bl_ind] = ant1\n",
    "                    red_groups_model[bl_ind] = red_ind\n",
    "                    continue\n",
    "\n",
    "        use_red_groups = [group for group in red_groups_data if group in red_groups_model]\n",
    "        use_data = np.zeros((len(use_red_groups), data.Nfreqs, data.Npols), dtype=complex)\n",
    "        use_model = np.zeros(\n",
    "            (len(use_red_groups), data.Nfreqs, data.Npols), dtype=complex\n",
    "        )\n",
    "        for group_ind, group in enumerate(use_red_groups):\n",
    "            use_data[group_ind, :, :] = data.data_array[np.where(red_groups_data == group)[0], :, :]\n",
    "            use_data[\n",
    "                np.where(data.flag_array[np.where(red_groups_data == group)[0], :, :])\n",
    "            ] = np.nan + 1j*np.nan\n",
    "            use_model[group_ind, :, :] = model.data_array[\n",
    "                np.where(red_groups_model == group)[0], :, :\n",
    "            ]\n",
    "            use_model[\n",
    "                np.where(data.flag_array[np.where(red_groups_data == group)[0], :, :])\n",
    "            ] = (np.nan + 1j * np.nan)\n",
    "\n",
    "        print(np.nanmean(np.abs(use_data - use_model)))\n",
    "        print(np.nanmean(np.abs(use_data)))\n",
    "        print(np.nanmean(np.abs(use_model)))\n",
    "        plt.scatter(np.real(use_data), np.imag(use_data), marker=\".\")\n",
    "        plt.scatter(\n",
    "            np.real(use_model), np.imag(use_model), marker=\".\"\n",
    "        )\n",
    "        plt.xlabel(\"visibility (real part)\")\n",
    "        plt.ylabel(\"visibility (imag part)\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
